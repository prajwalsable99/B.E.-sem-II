{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tV1DmxuXIA8W"
   },
   "source": [
    "**Whitespace based Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ninuLRfIWmA"
   },
   "source": [
    "Syntax : tokenize.WhitespaceTokenizer()\n",
    "\n",
    "Return : Return the tokens from a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MXMZ4mzceDyq",
    "outputId": "9c2379a8-53ba-45bb-c95d-f36bcc4613b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "Welcome to the I2IT-NLP Page. \n",
      " Good Morning \t\n",
      "\n",
      "Splitting using whitespece into separate tokens:\n",
      "['Welcome', 'to', 'the', 'I2IT-NLP', 'Page.', 'Good', 'Morning']\n"
     ]
    }
   ],
   "source": [
    "# import WhitespaceTokenizer() method from nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "     \n",
    "# Create a reference variable for Class WhitespaceTokenizer\n",
    "wt = WhitespaceTokenizer()\n",
    "\n",
    "# Create a string input\n",
    "text = \"Welcome to the I2IT-NLP Page. \\n Good Morning \\t\"\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)     \n",
    "# Use tokenize method\n",
    "tokenized_text = wt.tokenize(text)\n",
    "print(\"\\nSplitting using whitespece into separate tokens:\")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xheToBgAOHAf"
   },
   "source": [
    "**Punctuation-based tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yc66L2djPJw5",
    "outputId": "396e7994-a882-44af-aa1e-977a4d134545"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "Welcome to the I2IT-NLP Page. \n",
      " Good Morning \t\n",
      "\n",
      "Split all punctuation into separate tokens:\n",
      "['Welcome', 'to', 'the', 'I2IT', '-', 'NLP', 'Page', '.', 'Good', 'Morning']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "text = \"Welcome to the I2IT-NLP Page. \\n Good Morning \\t\"\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)\n",
    "result = WordPunctTokenizer().tokenize(text)\n",
    "print(\"\\nSplit all punctuation into separate tokens:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZXRMV3oRf7b"
   },
   "source": [
    "**Treebank Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akr3mMXyR4qk",
    "outputId": "5371a447-e568-4e90-a596-9b8b8cad83d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome', 'to', 'the', 'I2IT-NLP', 'Page.', 'Good', 'Morning']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    " \n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UUPDg6WSSFF"
   },
   "source": [
    "**Tweet Tokenizer**\n",
    "\n",
    "When we want to apply tokenization in text data like tweets, the tokenizers mentioned above canâ€™t produce practical tokens. Through this issue, NLTK has a rule based tokenizer special for tweets. We can split emojis into different words if we need them for tasks like sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fBg7Ks0OSVbv",
    "outputId": "9f712d9f-15e4-4d52-88db-d4a32c3194c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Who', 'is', 'your', 'favourite', 'cryptocurrency', 'influencer', '?', 'ðŸ—£', 'ðŸ†', 'Tag', 'them', 'below', '!', 'ðŸ‘‡']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenize = TweetTokenizer()\n",
    "sample_tweet = \"Who is your favourite cryptocurrency influencer? ðŸ—£ðŸ† Tag them below! ðŸ‘‡\"\n",
    "print(tweet_tokenize.tokenize(sample_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTJrhPFAMisl"
   },
   "source": [
    "**Multi-Word Expression Tokenizer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_mjT3cMjNYVW",
    "outputId": "19143200-48d8-4c7d-c582-395b6ae23166"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In', 'a_little', 'or', 'a_little_bit', 'or', 'a_lot', 'in_spite_of']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import MWETokenizer() method from nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "#from nltk.tokenize import  word_tokenize\n",
    "\n",
    "# Create a reference variable for Class MWETokenizer\n",
    "tokenizer = MWETokenizer([('a', 'little'), ('a', 'little', 'bit'), ('a', 'lot')])\n",
    "\n",
    "tokenizer.add_mwe(('in', 'spite', 'of'))\n",
    "tokenizer.tokenize('In a little or a little bit or a lot in spite of'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_LDNaecgRkol",
    "outputId": "4d3a36c1-444e-4ab1-9276-b4260814557e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Steven_Spielberg', 'is', 'an', 'American', 'writer', 'producer', 'director']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "tokenizer = MWETokenizer()\n",
    "tokenizer.add_mwe(('Steven', 'Spielberg'))\n",
    "tokenizer.tokenize('Steven Spielberg is an American writer producer director '.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJkQrfPQSSYY"
   },
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HWlwhtKQEUCF",
    "outputId": "7b51c728-0bca-4c67-bd1e-bdb1a989ab73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connector\n",
      "connect\n",
      "connect\n",
      "connect\n",
      "connect\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "example_words =[\"connector\",\"connection\",\"connects\",\"connecting\",\"connected\"]\n",
    "         \n",
    "#Next, we can easily stem by doing something like:\n",
    "for w in example_words:\n",
    "  print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcVk7qyKXHRI"
   },
   "source": [
    "**Snowball Stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wAGN75s2XLbE",
    "outputId": "602c63ab-3a16-4def-9804-c17f5379748c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous ---> generous\n",
      "generate ---> generat\n",
      "generously ---> generous\n",
      "generation ---> generat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball = SnowballStemmer(language='english')\n",
    "words = ['generous','generate','generously','generation']\n",
    "for word in words:\n",
    "    print(word,\"--->\",snowball.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sACRtj5tXS43"
   },
   "source": [
    "**Lancaster Stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AjMhLJiMXVfY",
    "outputId": "2d80177c-9250-4551-a21e-eef7e1e78edb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eat\n",
      "eats ---> eat\n",
      "eaten ---> eat\n",
      "puts ---> put\n",
      "putting ---> put\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lancaster = LancasterStemmer()\n",
    "words = ['eating','eats','eaten','puts','putting']\n",
    "for word in words:\n",
    "    print(word,\"--->\",lancaster.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivQXpxIhXZXt"
   },
   "source": [
    "**Regex Stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9LOieGQLXblB",
    "outputId": "16154fb1-eb3e-456e-9479-4515468e5c8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mass ---> mas\n",
      "was ---> was\n",
      "bee ---> bee\n",
      "computer ---> computer\n",
      "advisable ---> advis\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "words = ['mass','was','bee','computer','advisable']\n",
    "for word in words:\n",
    "    print(word,\"--->\",regexp.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESqsRT3KXhb-"
   },
   "source": [
    "**Porter Vs Snowball Vs Lancaster Vs Regex Stemmers**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "roEhJiRqXnXw",
    "outputId": "083ba09b-94df-46c5-814c-398e32c0a3f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      Snowball Stemmer    Lancaster Stemmer             Regexp Stemmer                          \n",
      "generous            gener               generous            gen                           generou                                 \n",
      "generate            gener               generat             gen                           generat                                 \n",
      "generously          gener               generous            gen                           generously                              \n",
      "generation          gener               generat             gen                           generation                              \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer(language='english')\n",
    "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "word_list = ['generous','generate','generously','generation']\n",
    "print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(\"Word\",\"Porter Stemmer\",\"Snowball Stemmer\",\"Lancaster Stemmer\",'Regexp Stemmer'))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(word,porter.stem(word),snowball.stem(word),lancaster.stem(word),regexp.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3FpzRvSYL7Z"
   },
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45oT2GO-YOwF"
   },
   "source": [
    "**Use any technique for lemmatization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhj7j-DfaQAm"
   },
   "source": [
    "**Using NLTK Library for Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J4cSj__tZ11R",
    "outputId": "63584a1b-e141-4b3a-ced7-15ce17949230"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9yahR4IcZv40",
    "outputId": "e6fa395f-12f5-4e2f-88a9-3dba7de0740b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
